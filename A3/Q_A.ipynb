{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b662749d-f773-401f-8828-2155349e38c1",
   "metadata": {},
   "source": [
    "# A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a2d234e-f6eb-48b7-a916-135286df75cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/04 14:12:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/04 14:12:43 WARN Utils: Service 'sparkDriver' could not bind on port 9999. Attempting port 10000.\n",
      "25/03/04 14:12:44 WARN StandaloneSchedulerBackend: Dynamic allocation enabled without spark.executor.cores explicitly set, you may get more executors allocated than expected. It's recommended to set spark.executor.cores explicitly. Please check SPARK-30299 for more details.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "spark_session = SparkSession.builder \\\n",
    "   .master(\"spark://192.168.2.156:7077\") \\\n",
    "   .appName(\"alexanderSundquist_A1\") \\\n",
    "   .config(\"spark.dynamicAllocation.enabled\", True) \\\n",
    "   .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True) \\\n",
    "   .config(\"spark.shuffle.service.enabled\", False) \\\n",
    "   .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"30s\") \\\n",
    "   .config(\"spark.cores.max\", 2) \\\n",
    "   .config(\"spark.driver.port\",9999)\\\n",
    "   .config(\"spark.blockManager.port\",10005)\\\n",
    "   .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "011ac85d-d754-4c75-844a-3d53d2441fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35f4b84f-e624-4907-93e0-3c25168cd731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Swedish and its English counterpart from HDFS\n",
    "en_file = spark_context.textFile(\"hdfs://192.168.2.156:9000/data/europarl/europarl-v7.sv-en.en\")\n",
    "sv_file = spark_context.textFile(\"hdfs://192.168.2.156:9000/data/europarl/europarl-v7.sv-en.sv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b49c981-caf5-49cc-8cd1-b19c8243b736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Resumption of the session'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_file.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d861f05-3f47-4340-a0cf-9ba92546103b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Återupptagande av sessionen'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_file.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4ec600e-8d7b-4115-bbe0-e0e0cc1ffe7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line count Swedish file 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================>                             (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line count English file 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"Line count Swedish file\", sv_file.count())\n",
    "print(\"Line count English file\", en_file.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf066ed-5cb0-4d08-8bee-6de11db25d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of partitions \n",
    "en_file.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44559f5d-4243-4174-9bf8-54f6e5925c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_file.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c21fc46-92f5-4795-9041-42d9668aa35a",
   "metadata": {},
   "source": [
    "# A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2909bc1-7e1e-499f-8148-fbbcc7b7e89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "återupptagande av sessionen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "first_line = sv_file.first()\n",
    "first_line_lower = first_line.lower()\n",
    "print(first_line_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f2e0e1-8b03-4278-a9e8-f7426bb059c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_special_char(file):\n",
    "    return file.map(lambda line: re.sub(\"[^A-Za-z0-9 -åäöÅÄÖ]+\",'', line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4096e1e2-ac77-48dd-a221-0f69b23a434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(file):\n",
    "    return file.map(lambda line: line.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "922c956e-7727-47b4-babe-6357764ce940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(file):\n",
    "    return file.map(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8240622e-287d-4366-a20e-71b111714796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swedish preprocessed:  [['återupptagande', 'av', 'sessionen'], ['jag', 'förklarar', 'europaparlamentets', 'session', 'återupptagen', 'efter', 'avbrottet', 'den', '17', 'december.', 'jag', 'vill', 'på', 'nytt', 'önska', 'er', 'ett', 'gott', 'nytt', 'år', 'och', 'jag', 'hoppas', 'att', 'ni', 'haft', 'en', 'trevlig', 'semester.'], ['som', 'ni', 'kunnat', 'konstatera', 'ägde', '\"den', 'stora', 'år', '2000-buggen\"', 'aldrig', 'rum.', 'däremot', 'har', 'invånarna', 'i', 'ett', 'antal', 'av', 'våra', 'medlemsländer', 'drabbats', 'av', 'naturkatastrofer', 'som', 'verkligen', 'varit', 'förskräckliga.'], ['ni', 'har', 'begärt', 'en', 'debatt', 'i', 'ämnet', 'under', 'sammanträdesperiodens', 'kommande', 'dagar.'], ['till', 'dess', 'vill', 'jag', 'att', 'vi,', 'som', 'ett', 'antal', 'kolleger', 'begärt,', 'håller', 'en', 'tyst', 'minut', 'för', 'offren', 'för', 'bl.a.', 'stormarna', 'i', 'de', 'länder', 'i', 'europeiska', 'unionen', 'som', 'drabbats.'], ['jag', 'ber', 'er', 'resa', 'er', 'för', 'en', 'tyst', 'minut.'], ['(parlamentet', 'höll', 'en', 'tyst', 'minut.)'], ['fru', 'talman!', 'det', 'gäller', 'en', 'ordningsfråga.'], ['ni', 'känner', 'till', 'från', 'media', 'att', 'det', 'skett', 'en', 'rad', 'bombexplosioner', 'och', 'mord', 'i', 'sri', 'lanka.'], ['en', 'av', 'de', 'personer', 'som', 'mycket', 'nyligen', 'mördades', 'i', 'sri', 'lanka', 'var', 'kumar', 'ponnambalam,', 'som', 'besökte', 'europaparlamentet', 'för', 'bara', 'några', 'månader', 'sedan.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English preprocessed:  [['resumption', 'of', 'the', 'session'], ['i', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'european', 'parliament', 'adjourned', 'on', 'friday', '17', 'december', '1999,', 'and', 'i', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period.'], ['although,', 'as', 'you', 'will', 'have', 'seen,', 'the', 'dreaded', \"'millennium\", \"bug'\", 'failed', 'to', 'materialise,', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful.'], ['you', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days,', 'during', 'this', 'part-session.'], ['in', 'the', 'meantime,', 'i', 'should', 'like', 'to', 'observe', 'a', \"minute'\", 's', 'silence,', 'as', 'a', 'number', 'of', 'members', 'have', 'requested,', 'on', 'behalf', 'of', 'all', 'the', 'victims', 'concerned,', 'particularly', 'those', 'of', 'the', 'terrible', 'storms,', 'in', 'the', 'various', 'countries', 'of', 'the', 'european', 'union.'], ['please', 'rise,', 'then,', 'for', 'this', \"minute'\", 's', 'silence.'], ['(the', 'house', 'rose', 'and', 'observed', 'a', \"minute'\", 's', 'silence)'], ['madam', 'president,', 'on', 'a', 'point', 'of', 'order.'], ['you', 'will', 'be', 'aware', 'from', 'the', 'press', 'and', 'television', 'that', 'there', 'have', 'been', 'a', 'number', 'of', 'bomb', 'explosions', 'and', 'killings', 'in', 'sri', 'lanka.'], ['one', 'of', 'the', 'people', 'assassinated', 'very', 'recently', 'in', 'sri', 'lanka', 'was', 'mr', 'kumar', 'ponnambalam,', 'who', 'had', 'visited', 'the', 'european', 'parliament', 'just', 'a', 'few', 'months', 'ago.']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "sv_preprocessed = remove_special_char(sv_file)\n",
    "sv_preprocessed = lower_case(sv_preprocessed)\n",
    "sv_preprocessed = tokenize(sv_preprocessed)\n",
    "\n",
    "en_preprocessed = remove_special_char(en_file)\n",
    "en_preprocessed = lower_case(en_preprocessed)\n",
    "en_preprocessed = tokenize(en_preprocessed)\n",
    "\n",
    "print(\"Swedish preprocessed: \", sv_preprocessed.take(10))\n",
    "print(\"English preprocessed: \", en_preprocessed.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c91c437c-180b-47db-8575-ce5184dfa3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line count Swedish file 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line count English file 1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"Line count Swedish file\", sv_preprocessed.count())\n",
    "print(\"Line count English file\", en_preprocessed.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593f2645-64e1-4347-b363-3e15e6565bf6",
   "metadata": {},
   "source": [
    "# A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e85c6cf-c667-4d77-bfc4-a696491ef995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_special_char(file):\n",
    "    return file.map(lambda line: re.sub(\"[^A-Za-z0-9 åäöÅÄÖ]+\",'', line))\n",
    "\n",
    "def lower_case(file):\n",
    "    return file.map(lambda line: line.lower())\n",
    "\n",
    "# Easier to use flatmap to break out of lists that are kept with map()\n",
    "def tokenize(file):\n",
    "    return file.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "sv_pre = remove_special_char(sv_file)\n",
    "sv_pre = lower_case(sv_pre)\n",
    "sv_pre = tokenize(sv_pre)\n",
    "\n",
    "en_pre = remove_special_char(en_file)\n",
    "en_pre = lower_case(en_pre)\n",
    "en_pre = tokenize(en_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24871159-623a-456a-9acd-6031a8b8b00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('resumption', 527), ('of', 1662006), ('i', 504521), ('declare', 1386), ('european', 270336)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:======================================>                   (2 + 1) / 3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('session', 771), ('efter', 42231), ('avbrottet', 293), ('december', 6019), ('vill', 131230)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# MapReduce to count occurences of each word\n",
    "\n",
    "en_words = en_pre.map(lambda w: (w,1))\n",
    "en_counts = en_words.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "sv_words = sv_pre.map(lambda w: (w,1))\n",
    "sv_counts = sv_words.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(en_counts.take(5))\n",
    "print(sv_counts.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a7eca2f-4d2e-4300-8613-3de2bd4446bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('återupptagande', 1),\n",
       " ('av', 1),\n",
       " ('sessionen', 1),\n",
       " ('jag', 1),\n",
       " ('förklarar', 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e504ece2-a8dc-44b8-bc3b-c3e8aef3c1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('att', 1709939),\n",
       " ('och', 1350379),\n",
       " ('i', 1054253),\n",
       " ('det', 952991),\n",
       " ('som', 917591),\n",
       " ('för', 915081),\n",
       " ('av', 740724),\n",
       " ('är', 701842),\n",
       " ('en', 636829),\n",
       " ('vi', 546072)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate 10 most reacurring words in sv file\n",
    "\n",
    "sorted_sv_counts = sv_counts.sortBy(lambda w: w[1], ascending=False)\n",
    "sorted_sv_counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57082538-3b1f-496d-b529-b15d2680e770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:======================================>                   (2 + 1) / 3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words= 41604741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Calculate total words in sv file\n",
    "\n",
    "from operator import add\n",
    "\n",
    "sv_words = sv_file.map(lambda line: line.split(\" \"))\n",
    "\n",
    "sv_word_counts = sv_words.map(lambda w: len(w))\n",
    "\n",
    "sv_total_words = sv_word_counts.reduce(add)\n",
    "\n",
    "print(f\"total words= {sv_total_words}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcbf5c83-b168-4e8c-b8b9-51a682825cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3505175),\n",
       " ('of', 1662006),\n",
       " ('to', 1543746),\n",
       " ('and', 1318369),\n",
       " ('in', 1088902),\n",
       " ('that', 839083),\n",
       " ('is', 774941),\n",
       " ('a', 774545),\n",
       " ('for', 538192),\n",
       " ('we', 526488)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate 10 most reacurring words in en file\n",
    "sorted_en_counts = en_counts.sortBy(lambda w: w[1], ascending=False)\n",
    "sorted_en_counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6619565-af61-4697-8965-307d8144ff68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words= 45778381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Calculate total words for en file\n",
    "from operator import add\n",
    "\n",
    "en_words = en_file.map(lambda line: line.split(\" \"))\n",
    "\n",
    "en_word_counts = en_words.map(lambda w: len(w))\n",
    "\n",
    "en_total_words = en_word_counts.reduce(add)\n",
    "\n",
    "print(f\"total words= {en_total_words}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abd64b-69da-4c9c-a026-c3c5d3dd7a22",
   "metadata": {},
   "source": [
    "# A4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be7257ac-42f3-4f44-9fdf-3f493735c203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "sv_1 = sv_preprocessed.zipWithIndex()\n",
    "en_1 = en_preprocessed.zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ee5572-3c4e-4b16-a0f9-1807922261d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_2 = sv_1.map(lambda x: (x[1], x[0]))\n",
    "en_2 = en_1.map(lambda x: (x[1], x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c95ddb2-5b63-4546-a297-ca9de7bbfaab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(20940,\n",
       "  (['det',\n",
       "    'jag',\n",
       "    'nu',\n",
       "    'tänker',\n",
       "    'säga',\n",
       "    'gäller',\n",
       "    'därför',\n",
       "    'bara',\n",
       "    'betänkandet',\n",
       "    'från',\n",
       "    'duff',\n",
       "    'och',\n",
       "    'voggenhuber.'],\n",
       "   ['so,',\n",
       "    'for',\n",
       "    'the',\n",
       "    'time',\n",
       "    'being,',\n",
       "    'what',\n",
       "    'i',\n",
       "    'am',\n",
       "    'about',\n",
       "    'to',\n",
       "    'say',\n",
       "    'applies',\n",
       "    'only',\n",
       "    'to',\n",
       "    'the',\n",
       "    'duff/voggenhuber',\n",
       "    'report.'])),\n",
       " (36885,\n",
       "  (['idn',\n",
       "    'med',\n",
       "    'en',\n",
       "    'europeisk',\n",
       "    'lista',\n",
       "    'är',\n",
       "    'förhastad',\n",
       "    'eller',\n",
       "    'till',\n",
       "    'och',\n",
       "    'med',\n",
       "    'knäpp.'],\n",
       "   ['the',\n",
       "    'idea',\n",
       "    'of',\n",
       "    'european',\n",
       "    'lists',\n",
       "    'is',\n",
       "    'premature,',\n",
       "    'if',\n",
       "    'not',\n",
       "    'hare-brained.'])),\n",
       " (43440,\n",
       "  (['herr',\n",
       "    'talman!',\n",
       "    'i',\n",
       "    'och',\n",
       "    'med',\n",
       "    'godkännandet,',\n",
       "    'utan',\n",
       "    'ändringar',\n",
       "    'i',\n",
       "    'rådets',\n",
       "    'gemensamma',\n",
       "    'ståndpunkt,',\n",
       "    'antar',\n",
       "    'europaparlamentet',\n",
       "    'utmaningen',\n",
       "    'från',\n",
       "    'de',\n",
       "    'förändringar',\n",
       "    'som',\n",
       "    'pågår,',\n",
       "    'de',\n",
       "    'krav',\n",
       "    'som',\n",
       "    'ställs',\n",
       "    'från',\n",
       "    'den',\n",
       "    'nya',\n",
       "    'ekonomin,',\n",
       "    'och',\n",
       "    'ställer',\n",
       "    'äntligen',\n",
       "    'upp',\n",
       "    'regler',\n",
       "    'för',\n",
       "    'användningen',\n",
       "    'av',\n",
       "    'internet',\n",
       "    'som',\n",
       "    'ett',\n",
       "    'instrument',\n",
       "    'för',\n",
       "    'utbyte',\n",
       "    'av',\n",
       "    'varor',\n",
       "    'och',\n",
       "    'tillhandahållande',\n",
       "    'av',\n",
       "    'tjänster.'],\n",
       "   ['mr',\n",
       "    'president,',\n",
       "    'with',\n",
       "    'the',\n",
       "    'adoption,',\n",
       "    'without',\n",
       "    'amendments,',\n",
       "    'of',\n",
       "    'the',\n",
       "    'common',\n",
       "    'position',\n",
       "    'of',\n",
       "    'the',\n",
       "    'council,',\n",
       "    'the',\n",
       "    'european',\n",
       "    'parliament',\n",
       "    'is',\n",
       "    'rising',\n",
       "    'to',\n",
       "    'the',\n",
       "    'challenges',\n",
       "    'of',\n",
       "    'the',\n",
       "    'current',\n",
       "    'changes,',\n",
       "    'to',\n",
       "    'the',\n",
       "    'demands',\n",
       "    'of',\n",
       "    'the',\n",
       "    'new',\n",
       "    'economy,',\n",
       "    'at',\n",
       "    'last',\n",
       "    'setting',\n",
       "    'rules',\n",
       "    'in',\n",
       "    'place',\n",
       "    'governing',\n",
       "    'the',\n",
       "    'use',\n",
       "    'of',\n",
       "    'the',\n",
       "    'internet',\n",
       "    'for',\n",
       "    'trade',\n",
       "    'and',\n",
       "    'the',\n",
       "    'provision',\n",
       "    'of',\n",
       "    'services.']))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_en_3 = sv_2.join(en_2)\n",
    "sv_en_3.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af6c7137-d57f-4529-8d0b-601e7049509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_en_4 = sv_en_3.filter(lambda x: x[1][1] != [''] and x[1][0] != [''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2ecbeb8-8037-4f53-8678-f624d2b6fd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words= 3696846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Calculate total words in sv file\n",
    "\n",
    "from operator import add\n",
    "\n",
    "word_count = sv_en_4.map(lambda w: len(w))\n",
    "\n",
    "total_words = word_count.reduce(add)\n",
    "\n",
    "print(f\"total words= {total_words}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2200e5fd-f70a-4c16-abf8-99aa8cfbf845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line count sv en removed empty lines 1848423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"Line count sv en removed empty lines\", sv_en_4.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f378a54-f0e2-4fa1-8c06-5c9ddd095dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line count sv en not removed empty lines:  1862234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"Line count sv en not removed empty lines: \", sv_en_3.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3677cd0-e71e-48d9-8f29-3aa2e2717b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:==============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(sv_en_3.count()-sv_en_4.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eeed9c4a-cf7e-4c00-a654-28de7b91cdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "2462"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_file.filter(lambda x: x == \"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e085a14c-2a3d-4094-96cd-5f488bf7599c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "2462"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_preprocessed.filter(lambda x: x == ['']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75c48159-1175-4dba-a727-31c1cd6c162c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "11349"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_file.filter(lambda x: x == \"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc04e516-2a7f-4f3d-86c8-e271786d7add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "11349"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_preprocessed.filter(lambda x: x == ['']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03eac94b-daa7-4b7e-bd17-3f4a7042c213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Återupptagande av sessionen',\n",
       " 'Jag förklarar Europaparlamentets session återupptagen efter avbrottet den 17 december. Jag vill på nytt önska er ett gott nytt år och jag hoppas att ni haft en trevlig semester.',\n",
       " 'Som ni kunnat konstatera ägde \"den stora år 2000-buggen\" aldrig rum. Däremot har invånarna i ett antal av våra medlemsländer drabbats av naturkatastrofer som verkligen varit förskräckliga.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_file.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1d941a9-46e0-4d3f-95dc-56bc7ff732ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['återupptagande', 'av', 'sessionen'],\n",
       " ['jag',\n",
       "  'förklarar',\n",
       "  'europaparlamentets',\n",
       "  'session',\n",
       "  'återupptagen',\n",
       "  'efter',\n",
       "  'avbrottet',\n",
       "  'den',\n",
       "  '17',\n",
       "  'december.',\n",
       "  'jag',\n",
       "  'vill',\n",
       "  'på',\n",
       "  'nytt',\n",
       "  'önska',\n",
       "  'er',\n",
       "  'ett',\n",
       "  'gott',\n",
       "  'nytt',\n",
       "  'år',\n",
       "  'och',\n",
       "  'jag',\n",
       "  'hoppas',\n",
       "  'att',\n",
       "  'ni',\n",
       "  'haft',\n",
       "  'en',\n",
       "  'trevlig',\n",
       "  'semester.'],\n",
       " ['som',\n",
       "  'ni',\n",
       "  'kunnat',\n",
       "  'konstatera',\n",
       "  'ägde',\n",
       "  '\"den',\n",
       "  'stora',\n",
       "  'år',\n",
       "  '2000-buggen\"',\n",
       "  'aldrig',\n",
       "  'rum.',\n",
       "  'däremot',\n",
       "  'har',\n",
       "  'invånarna',\n",
       "  'i',\n",
       "  'ett',\n",
       "  'antal',\n",
       "  'av',\n",
       "  'våra',\n",
       "  'medlemsländer',\n",
       "  'drabbats',\n",
       "  'av',\n",
       "  'naturkatastrofer',\n",
       "  'som',\n",
       "  'verkligen',\n",
       "  'varit',\n",
       "  'förskräckliga.']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_preprocessed.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfda135-4431-444e-b236-d7e0e0e68c55",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "142980df-f43f-41cd-ac43-bc4900f79af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "200796"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_en_5 = sv_en_4.filter(lambda x: len(x[1][1]) < 10 and len(x[1][0]) < 10) \n",
    "sv_en_5.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3798bc-24d9-4ed5-ae16-9e5eb2e5ea0c",
   "metadata": {},
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a258c262-0fbe-4500-94c7-41c9f54f4cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:===============================================>           (4 + 1) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line count sv_en_6 73844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "sv_en_6 = sv_en_5.filter(lambda x: len(x[1][1]) == len(x[1][0])) \n",
    "print(f\"Line count sv_en_6 {sv_en_6.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e14149-e624-4240-b6cd-09fc998a4789",
   "metadata": {},
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7b9a492-6fb5-47f9-98f7-0b6cbb6d4871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(45385,\n",
       "  (['den', 'första', 'frågan', 'handlar', 'om', 'kommittförfarandet.'],\n",
       "   ['the', 'first', 'question', 'refers', 'to', 'commitology.'])),\n",
       " (206300,\n",
       "  (['omröstningen',\n",
       "    'kommer',\n",
       "    'att',\n",
       "    'äga',\n",
       "    'rum',\n",
       "    'i',\n",
       "    'morgon',\n",
       "    'kl.',\n",
       "    '11.00.'],\n",
       "   ['the', 'vote', 'will', 'take', 'place', 'tomorrow', 'at', '11', 'a.m.'])),\n",
       " (269015,\n",
       "  (['parlamentarisk',\n",
       "    'kontroll',\n",
       "    'och',\n",
       "    'en',\n",
       "    'medlagstiftandebefogenhet',\n",
       "    'är',\n",
       "    'också',\n",
       "    'oumbärligt.'],\n",
       "   ['central',\n",
       "    'is',\n",
       "    'therefore',\n",
       "    'also',\n",
       "    'parliamentary',\n",
       "    'control',\n",
       "    'and',\n",
       "    'co-legislation.']))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_en_6.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd5f4dad-2140-428f-915e-ef4cee1780e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_lists(list_1, list_2):\n",
    "    translation_list = []\n",
    "    for last, first in zip(list_1, list_2):\n",
    "        pair_list = [last,first]\n",
    "        translation_list.append(pair_list)\n",
    "    return translation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01edbe16-7394-4703-8942-af6fcd749d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[['jag', 'i'],\n",
       "  ['är', 'am'],\n",
       "  ['ledsen', 'sorry'],\n",
       "  ['för', 'about'],\n",
       "  ['detta.', 'that.']],\n",
       " [['även', 'bulgaria'],\n",
       "  ['bulgarien', 'has'],\n",
       "  ['har', 'also'],\n",
       "  ['ansökt', 'applied'],\n",
       "  ['om', 'to'],\n",
       "  ['medlemskap', 'join'],\n",
       "  ['i', 'the'],\n",
       "  ['europeiska', 'european'],\n",
       "  ['unionen.', 'union.']],\n",
       " [['jag', 'i'],\n",
       "  ['vill', 'thank'],\n",
       "  ['verkligen', 'mrs'],\n",
       "  ['tacka', 'kauppi'],\n",
       "  ['fru', 'very'],\n",
       "  ['kauppi', 'much'],\n",
       "  ['för', 'for'],\n",
       "  ['hennes', 'her'],\n",
       "  ['betänkande.', 'report.']]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_en_7 = sv_en_6.map(lambda x: zip_lists(x[1][0], x[1][1]))\n",
    "sv_en_7.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb25aa81-cd4b-4a86-a137-17a426f3843f",
   "metadata": {},
   "source": [
    "## 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48162899-7239-4aa7-83b8-ebff99804532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/04 14:44:29 ERROR TaskSetManager: Task 0 in stage 29.0 failed 4 times; aborting job\n",
      "[Stage 29:>                                                         (0 + 1) / 5]"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 4 times, most recent failure: Lost task 0.3 in stage 29.0 (TID 77) (192.168.2.60 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 840, in func\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 3983, in combineLocally\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 840, in func\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 3983, in combineLocally\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#wp_map.take(5)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m wp_counts \u001b[38;5;241m=\u001b[39m wp_map\u001b[38;5;241m.\u001b[39mreduceByKey(\u001b[38;5;28;01mlambda\u001b[39;00m a, b: a \u001b[38;5;241m+\u001b[39m b)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mwp_counts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Calculate 10 most reacurring words in sv file\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#sorted_sv_counts = sv_counts.sortBy(lambda w: w[1], ascending=False)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#sorted_sv_counts.take(10)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2852\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2854\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 2855\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2857\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   2858\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   2508\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2510\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 4 times, most recent failure: Lost task 0.3 in stage 29.0 (TID 77) (192.168.2.60 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 840, in func\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 3983, in combineLocally\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 840, in func\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 3983, in combineLocally\n  File \"/home/ubuntu/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 258, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# MapReduce to count occurences of each word pair\n",
    "from operator import add\n",
    "\n",
    "word_pairs = sv_en_7.flatMap(lambda wp : wp)\n",
    "\n",
    "wp_map = word_pairs.map(lambda wp: (wp,1))\n",
    "\n",
    "#wp_map.take(5)\n",
    "\n",
    "wp_counts = wp_map.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(wp_counts.take(5))\n",
    "\n",
    "# Calculate 10 most reacurring words in sv file\n",
    "\n",
    "#sorted_sv_counts = sv_counts.sortBy(lambda w: w[1], ascending=False)\n",
    "#sorted_sv_counts.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe786f-6e2b-438f-ab92-c062c6903be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_counts = sv_words.reduceByKey(lambda a, b: a + b)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
